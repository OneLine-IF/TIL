## 퀴즈 대비 이론 재 정리
### 1주차
- Time Complexity
	- A finite set of statements that guarantees an optimal solution in finite interval of time
	- Run in less time 
	- Consume less memory
- The efficiency of an algorithm is a measure of the amount of resources consumed in solving a problem of size n 
- the most obivous way to measure the efficiency of an algorithm is to run it and measure how much processor time is needed
- Generally time grows with size of input. plus Running time is measured in terms of number of steps/primitive operations performed
- Running time is measured by number of steps/primitive operations performed, steps means elementary operation like -,+,* etc...
- we will measure number of steps taken in terms of size of input
- Asymptotic Complexity 
	- As N gets large, concentrate on the highest order term
	- If 5N+3
		- Drop lower order terms such as +3
		- Drop the constant coefficient (5) of the highest order term i.e.N
		- Then Finally Time complexity is O(N)
	- This gives us an approximation of the complexity of the algorithm
	- Ignores lots of details, concentrate on the bigger picture
- Big Oh Notation
	- As inputs get larger, any algorithm of a smaller order will be more efficient than an algorithm of a larger order
	- Even though it is correct to say 7n-3 is O(n^3), a better statement is 7n-3 is O(n), that is, one should make the approximation as tight as possible
	- Drop lower order terms and constant factors
		- 7n is O(n)
		- 8n^2logn+5n^2 is O(n^2logn)
	- size does matter
- Constant time statements
	- Simplest case O(1) time statements
		- Assignment statements of simple data types(int x = y)
		- Arithmetic operations(x = 5 * y + 4 - z)
		- Array referencing(A[i] = 5;)
		- Array assignment(v(j),A[j] = 5)
		- Most conditional tests(if(x<12))
	- Analyzing Loops
		- How many iterations are performed?
		- How many steps per iteration?
		- ex) int sum = 0,j; / for(j = 0; j < N; j++) sum = sum + j;
		- then in the ex) Loop executes N times, 4 = O(1) steps per iteration
		- Total time is N * O(1) = O(N * 1) = O(N)
		- If for loop execute 100 times then total time is 100 * O(1) = O(100) = O(1)
	- Analyzing Nested Loops
		- If in the double loop then analyze outer, inner loop
		- ex) for(j = 0; j < N; j++)
		-		for(k = N; k > 0; k--)
		-			sum += k + j;
		- Start with outer loop
		- How many iteration? N
		- How much time per iteration? Need to evaluate inner loop
		- Inner loop uses O(N) times
		- Total time is N * O(N) = O(N * N) = O(N^2)
		- main concept is anlayze inner and outer loop together
	- Analyzing Sequence of Statements
		- For a sequence of statements, compute their complexity functions individually and add them up
		- ex1) for(j = 0; j < N; j++)
		- 		for(k = 0; k < j; k++)
		-			sum = sum + j * k; 
		- ex1) time complexity is O(N^2)
		- ex2) for(i = 0; i < N; i++)
		-		sum = sum - i; 
		- ex2) time complexity is O(N)
		- ex3) cout<< "Sum="<< sum;
		- ex3) time complexity is O(1)
		- Total cost is O(N^2)+O(N)+O(1)=O(N^2) / sum rule
	- Analyzing Conditional Statements
		- if(condition) statement1;
		- else statement2;
		- where statement1 runs in O(N) time and statement2 runs in O(N^2) time
		- We use worst case complexity: among all inputs of size N, that is the maximum running time?
		- The analysis for the example above is O(N^2)

### 2주차
- Abstract Data Types(ADT)
	- a collection of related data items together with an associated set of operations
	- What, not how is the foucs
	- Consist of storage structures to store the data items and algorithms for the basic operations
- Each byte (or word) has an address making it possible to store and retrieve contents of any given memory location
- Queues
	- A Queue is a special kind of list, where items are inserted at one end(the rear) And deleted at the other end(the front)
	- First In First Out(FIFO)
	- Stack is insertion go at the end of the list 
	- Application of Queues
		- Operating system
		- Communication Software
	- Static -> implemented by an array
	- Dynamic -> implemented by an linked list
	- A pointer Implementation of Queues
		- Front -> A pointer to the first element of the queue
		- Rear -> A pointer to the last element of the queue
- Circular Queues
	- In circular queues Q[1] following Q[n].
	- A circular queue is one in which the insertion of a new element is done at the very first location of the queue if the last location at the queue is full
- Deques
	- A deque is a homogeneous list in which elements can be added or inserted(called push operation) and deleted or removed from both the ends (which is called pop opeartion)
	- we can add a new element at the rear or front end and also we can remove an element from both front and rear end. Hence it is called Double Ended Queue
	- Input restricted deque
		- a deque, which allows insertion at only 1 end, rear end, but allows deletion at both ends, rear and front end of the lists
	- Output restricted deque
		- a deque, which allows deletion at only one end, front end, but allows insertion at both ends, rear and front ends, of the lists
- Priority Queues
	- A priority queue is a data structure in which each element is assigned a priority
	- The priority of the element will be used to determine the order in which the elements will be processed
	- An element with higher priority is processed before an element with a lower priority
	- Two elements with the same priority are processed on a first-come-first-served(FCFS) basis
	- Every time when an element has to be removed from the list, the element with the highest priority will be searched and removed
	- While a sorted list takes O(n) time to insert an element in the list, it takes only O(1) time to delete an element
	- On the contrary, an unsorted list will take O(1) time to insert an element and O(n) time to delete an element from the list
	- both these techniques are inefficient and ususally a blend of these two approaches is adopted that takes roughly O(logn) time or less
	- In the linked list, the new node is inserted before the node with the lower priority, However if these exists an element that has the same priority as the new element, the new element is inserted after that element

### 3주차
- Sorting means arranging the elements of an array so that they are placed in some relevant order which may be either ascending or descending
- When analyzing the performance of different sorting algorithms, the practical considerations would be the following
	- Number of sort key comparisons that will be performed
	- Number of times the records in the list will be moved
	- Best case performance
	- Worst case performance
	- Average case performance
	- Stability of the sorting algorithm where stability means that equivalent elements or records retain their relative positions even after sorting is done 
- Insertion Sort
	- It inserts each item into its proper place in the final list 
	- moving the current data element past the already sorted values and repeatedly interchanging it with the preceding value until it is in its correct place 
	- Insertion sort is less efficient as compared to other more advanced algorithms such as quick sort, heap sort, and merge sort
	- Insertion sort, the best case occurs when the array is already sorted, In this case the running time of the algorithm has a linear running time, O(n)
	- the worst case of the insertion sort algorithm occurs when the array is sorted in the reverse order, In the worst case, insertion sort has a quadratic running time O(n^2)
	- in the average case, the insertion sort algorithm will have to make at least (k-1)/2 comparisons, Thus the average case also has a quadratic running time
	- easy and efficient to use on small sets of data
	- It can be efficiently implemented on data sets that are already substantially sorted
	- It performs better than algorithms like selection sort and bubble sort. And simpler than shell sort
- Selection Sort
	- Selection sort is a sorting algorithm that has a quadratic running time complexity of O(n^2), inefficient to be used on large lists 
	- it is noted for its simplicity, in certain situations performance advantages 
	- Selection sort is a sorting algorithm that is independent of the original order of elements in the array
- Practical Use of Queues
	- First-Come-First-Served Scheduling, Gantt Charts, Gantt Charts for Algorithm Evaluation, Shortest-Job-First Scheduling, Preemption and SJF Scheduling, Priority Scheduling, Starvation and Aging, Round-Robin Scheduling

### 4주차
- A heap is a specialized tree-based data structure, and implementation of priority queues
- Full binary tree 
	- a binary tree in which each node is either a leaf or has degree exactly 2 
- Complete binary tree
	- a binary tree in which all leaves are on the same level and all internal nodes have degree 2 
- Height of a node 
	- the number of edges on the longest simple path from the node down to a leaf
- Level of a node 
	- the length of a path from the root to the node
- Height of tree
	- height of root node
- Useful Properties
	- there are at most 2^l nodes at level (or depth) l of a binary tree
	- A binary tree with height d has at most 2^(d+1)-1 nodes
	- A binary tree with n nodes has height at least [lgn]
- A heap is a nearly complete binary tree with the following two properties
	- Structural property:all levels are full, except possibly the last one, which is filled from left to right
	- Order(heap) property:for any node x Parent(x)>=x
	- A heap is a binary tree that is filled in order
	- every node satisfies the heap property which states that 
		- If B is a child of A, THEN Key(A)>=Key(B)
	- elements at every node will be either greater than or equals to the element at its left and right child, root node has the highest key value in the heap -> Max-Heap
	- elements at every node will be either less than or equal to the element at its left and right child, root has the lowest key value 
	-> Min-Heap
- Binary Heaps
	- If an element is at position i in the array, then its left child is stored at position 2i and its right child at position 2i+1
	- Conversly, an element at position i has its parent stored at postion i/2
	- all the levels of the tree except the last level are completely filled
	- The height of a binary tree is given as log_2(n), where n is the number of elements
- Max-heaps(largest element at root), have the max-heap property, for all nodes i, excluding the root A[Parent(i)]>=A[i]
- Min-heaps(smallest element at root), habe the min-heap property, for all nodes i, excluding the root A[Parent(i)]<=A[i]
- Adding/Deleting Nodes 
	- New nodes are alwayws inserted at the bottom level(left to right)
	- new value rise to its appropriate place in H so that H now becomes a heap as well, To do this, compare the new value with its parent to check if they are in the correct order
	- Insertion in Binary Heaps
		- to build a heap with 9 elements, use a for loop that executes 9 times and in each pass, a single value is inserted, Then complexity of this algorithm in the average case is O(1), this because a binary heap has O(logn) height
		- In the worst case, insertion of a single value may take O(logn)time, to build a heap of n elements, the algorithm will execute in O(nlogn) time
	- Nodes are removed from the bottom level(right to left)
	- Replace the root node's value with the last node's value, Delete the last node, Sink down the new root node's value so that H statisfies the heap property
- Maintain/Restore the max-heap property
	- Max-Heapify
	- It traces a path from the root to a leaf(longest path length h)
	- At each level, it makes exactly 2 comparisons
	- Total number of comparisons is 2h
	- Running time is O(h) or O(lgn)
- Create a max-heap from an unordered array
	- Build-Max-Heap
	- n=length[A], for i <- [n/2] downto 1, do Max-Heapify
	- Max-heapify is O(logn), and iterate it N times then Running time is O(nlogn)
- Sort an array in place
	- Heapsort
	- Build-max-heap -> O(n), and max-heapify and swap the value iterate n-1 times, then Running time is O(nlogn)
- Priority queues
	- a better way to implement a priority queue is by using a binary heap which allows both enqueue and dequeue of elements in O(logn) time
	- Heap-Maximum
		- return the largest element of the heap -> running time O(1)
	- Heap-Extract-Max
		- Extract the largest element of the heap(return the max value and also remove that element from the heap)
		- Running time O(logn) because it uses max-heapify
	- Heap-Increase-Key
		- Increases the key of an element i in the heap
		- Running time O(lgn) -> because it uses binary heap 
	- Max-Heap-Insert
		- Inserts a new element into a max-heap
		- Running time O(lgn) -> it uses binary heap at traversal in binary heap then lgn
- In the heap average O(lgn)

### 5주차
- Bubble Sort
	- Bubble sort is a very simple method that sorts the array elements by repeatedly moving the largest element to the highest index position of the array segment
	- In Bubble sorting, consecutive adjacent pairs of elements in the array are compared with other
	- to compute the complexity of bubble sort, we need to calculate the total number of comparisons
	- f(n)=(n-1)+(n-2)+(n-3)+...+3+2+1 -> n(n-1)/2
	- f(n)=n^2/2+O(n)=O(n^2)
	- Therefore, the complexity of bubble sort algorithm is O(n^2)
	- If the best case, array is already sorted, the optimied bubble sort will take O(n) times
- Merge Sort
	- Divide means partitioning the n-element array to be sorted into two sub-arrays of n/2 elements
	- If A is an array containing zero or one element, then it is already sorted
	- However, if there are more elements in the array, divide A into two sub-arrays, A1, and A2, each containing about half of the elements of A
	- Conquer means sorting the two sub-arrays recursively using merge sort
	- Combine means merging the two sorted sub-arrays of size n/2 to produce the sorted array of n elements
	- While the merge sort algorithm recursively divides the list into smaller lists, the merge algorithm conquers the list to sort the elements in individual lists, Finally the smaller lists are merged to form on list
	- The running time of merge sort in the average case and the worst case can be given as O(nlogn), Although merge sort has an optimal time complexity, it needs an additional space of O(n) for the temporary array TEMP
- Quick Sort
	- Quick sort is a widely used sorting algorithm that makes O(nlogn) comparison in the average case to sort an array of n elements 
	- in the worst case, it has a quadratic running time gives as O(n^2)
	- quick sort algorithm is faster than other O(nlogn) algorithms, because can minimize the probability of requiring quadratic time
	- Like merge sort, works by using a divide-and-conquer strategy to divide a single unsorted array into two smaller sub-arrays
	- Select an element pivot from the array elements -> Rearrange the elements in the array in such a way that all elements that are less than the pivot appear before the pivot and all elements greater than the pivot element come after it. After such a partitioing the pivot is placed in its final postion -> Recursively sort the two sub-arrays thus obtained
	- the base case of the recursion occurs when the array has zero or one element because in that case the array is already sorted
	- After each iteration, one element(pivot) is always in its final position, Hence with every iteration, there is one less element to be sorted in the array
- Radix Sort
	- Radix sort is a linear sorting algorithm for integers and uses the concept of sorting names in alphabetical order, also known as bucket sort 
	- In this case the radix sort algorithm is called a total of k times, the inner loop is executed n times, Hence the entire radix sort takes O(kn) times to execute, then when radix sort is applied on a data set of finite size the runs in O(n) asymptotic time
- Shell Sort
	- elements are sorted in multiple passes and in each pass, data are taken with smaller and smaller gap sizes, the last step of shell sort is a plain insertion sort.

### 6주차
- Dictionary ADT
	- Stores values associated with user-specified keys
- Binary Search
	- Similar to the high-low game
	- at each step, the number of candidate items is halved
	- terminates after O(log n) steps
- Search Tables
	- we store the items of the dictionary in array-based sequence, sorted by key
	- find takes O(logn) time, using binary search
	- insert takes O(n) time since in the worst case we have to shift n/2 items to make room for the new item
	- remove take O(n) time since in the worst case we have to shift n/2 items to compact the items after the removal
- Binary Search Trees
	- a binary tree storing keys(or key-value entries) at its internal nodes and satisfying the following property
		- Let u, v and w be three nodes such that u is in the left subtree of v and w is in the right subtree of v we have key(u)<=key(v)<=key(w) 
		- An inorder traversal of a binary search trees visits the keys in increasing order
	- Basically Binary tree to search is takes O(n) but Binary search tree(BST) is in bst rules then in the searching takes O(logn) times
	